# RAGEN WebShop Configuration - PPO (Following RAGEN Paper Exactly)
# Based on: "RAGEN: Understanding Self-Evolution in LLM Agents via Multi-Turn RL"
# Paper settings: Section 3.2 (Training Settings) and Figure 3 (WebShop results)

model:
  name: "Qwen/Qwen2.5-3B-Instruct"  # Paper uses 3B variant for WebShop
  ref_model: "Qwen/Qwen2.5-3B-Instruct"
  max_length: 2048
  sft_max_length: 512
  device: "cuda"

# PPO Configuration (RAGEN paper specifications)
trainer:
  type: "ppo"  # Use PPO instead of A*-PO

ppo:
  # RAGEN paper specifications (Section 3.2)
  learning_rate: 0.00001  # 1e-5 (same as A*-PO for fair comparison)
  clip_epsilon: 0.2  # Standard PPO clipping
  value_loss_coef: 0.5  # Value function loss weight
  entropy_coef: 0.001  # β=0.001 (RAGEN paper)
  max_grad_norm: 1.0  # Gradient clipping

  # GAE parameters (RAGEN paper: γ=1.0, λ=1.0)
  gamma: 1.0  # Discount factor
  gae_lambda: 1.0  # GAE lambda

  # Response format penalty (RAGEN paper: -0.1)
  format_penalty: -0.1

# Rollout Configuration (RAGEN paper: P=8 prompts, N=16 rollouts per prompt)
rollout:
  prompts_per_batch: 8  # P=8 initial states per batch
  rollouts_per_prompt: 16  # N=16 trajectories per prompt
  max_turns: 5  # Up to 5 turns (RAGEN paper)
  max_actions_per_turn: 10  # Up to 10 actions per turn (RAGEN paper)

# Sampling during training
sampling:
  temperature: 0.9  # Higher temp for exploration during training
  top_p: 0.9
  top_k: 0

training:
  # RAGEN paper: 100-200 rollout-update iterations
  num_epochs: 1  # We handle iterations differently
  max_steps: 100  # 100 rollout-update iterations (can extend to 200)
  eval_every: 20  # Evaluate every 20 steps
  save_every: 20  # Save checkpoints every 20 steps
  batch_size: 8  # E=8 batch size for updates (P*N=8*16=128 rollouts)
  num_update_loops: 1  # L=1 loop per step

environment:
  type: "webshop"
  max_turns: 5  # RAGEN uses 5 turns for WebShop
  num_products: 100  # WebShop with 100 products

# Evaluation (RAGEN paper: 256 fixed prompts, T=0.5)
evaluation:
  num_episodes: 256  # Fixed evaluation set size
  temperature: 0.5  # Lower temperature for evaluation
  truncate_after_turns: 5

# StarPO-S Stabilization (Optional - can enable if training collapses)
# RAGEN paper: Filtering low-variance trajectories improves stability
uncertainty_filtering:
  enabled: true  # ENABLED - trajectory filtering for stable training
  keep_percent: 0.50  # Keep top 50% high-variance prompts
  start_step: 20  # Start after warm-up

data:
  train_size: 100  # Training tasks
  eval_size: 25  # Evaluation tasks

logging:
  use_wandb: false
  log_every: 2  # RAGEN paper: log every 2 steps

seed: 42

# Curriculum Learning (ENABLED for better learning)
# Start with fewer products, gradually increase difficulty
curriculum:
  enabled: true
  check_every: 20  # Check for progression every 20 steps
  stages:
    - products: 25  # Start with 25 products (easier)
      threshold: 0.05  # Need 5% success to progress (lowered from 15%)
    - products: 50  # Medium difficulty
      threshold: 0.10  # Need 10% success to progress (lowered from 20%)
    - products: 100  # Full difficulty
      threshold: 0.15  # Target 15% success (lowered from 25%)
