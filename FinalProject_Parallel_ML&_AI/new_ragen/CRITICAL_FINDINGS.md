# ðŸš¨ CRITICAL FINDINGS: Why new_ragen Doesn't Work

## TL;DR - The Root Cause

**TinyZero and new_ragen implement COMPLETELY DIFFERENT ALGORITHMS**, despite both claiming to be "A*-PO":

| | TinyZero (WORKS âœ…) | new_ragen (FAILS âŒ) |
|---|---|---|
| **Algorithm** | Weighted Supervised Learning | REINFORCE Policy Gradient |
| **Training** | Teacher-forcing on completions | Online trajectory rollouts |
| **Loss** | Weighted CE: `(CE_loss * weights).mean()` | Policy Gradient: `-(log_prob * adv).mean()` |
| **Task** | Math problems (simpler) | WebShop (complex multi-turn) |
| **Curriculum** | Yes (updates every 25 steps) | No |
| **Rewards** | Binary (0/1) for V*, Dense for training | Dense shaped rewards |
| **Reference Model** | Yes (for KL regularization) | No |

---

## The Fundamental Issue

### What You Thought You Were Implementing
A*-PO as described in the RAGEN paper using StarPO-style policy optimization.

### What You Actually Implemented
Two different algorithms:
1. **TinyZero**: Advantage-Weighted Supervised Learning (like AWR or AWAC)
2. **new_ragen**: REINFORCE with advantage baseline (like vanilla policy gradient)

### Why This Matters
These algorithms have VERY different properties:
- **Weighted SFT (TinyZero)**: Low variance, fast convergence, stable gradients
- **REINFORCE (new_ragen)**: High variance, slow convergence, unstable gradients

On **simple tasks** (TinyZero math): REINFORCE might work
On **complex multi-turn tasks** (WebShop): REINFORCE struggles without extensive tuning

---

## Detailed Comparison

### 1. Core Algorithm Difference

**TinyZero's "A*-PO" (Really: Advantage-Weighted SFT)**:
```python
# Step 1: Generate completions
generated_texts = policy.generate(prompts, do_sample=True)

# Step 2: Compute advantages
rewards = compute_reward(generated_texts, problems)
V_star = compute_V_star(prompts)  # From reference model samples
advantages = rewards - V_star

# Step 3: Convert advantages to positive weights
adv_norm = (advantages - adv_mean) / adv_std
adv_norm = adv_norm.clamp(-3.0, 3.0)
weights = (adv_norm + 1.0).clamp(0.1, 5.0)  # â† ALL POSITIVE!

# Step 4: Teacher-forced training
input_ids = concat(prompt_tokens, completion_tokens)
labels = mask_prompt_tokens(input_ids)
logits = policy.model(input_ids)
per_ex_ce_loss = compute_ce_loss(logits, labels)  # [B]

# Step 5: Weight each example
weighted_losses = per_ex_ce_loss * weights
loss = weighted_losses.mean()

# Step 6: Add KL regularization
kl_loss = compute_kl(policy_logits, ref_logits, labels)
total_loss = weighted_losses.mean() + kl_coef * kl_loss.mean()
```

**Key Properties**:
- âœ… Low variance (supervised learning gradient)
- âœ… Stable training (all weights positive)
- âœ… Efficient (no need to recompute log probs)
- âœ… Works like SFT but with smart example weighting

**new_ragen's A*-PO (Really: REINFORCE)**:
```python
# Step 1: Collect trajectory
trajectory = []
obs = env.reset()
for step in range(max_steps):
    action, log_prob = policy.generate_action(obs, compute_log_prob=True)
    obs, reward, done = env.step(action)
    trajectory.append((obs, action, log_prob))
    total_reward += reward

# Step 2: Compute advantage
V_star = value_cache.get(instruction)
advantage = total_reward - V_star

# Step 3: Policy gradient
total_log_prob = sum(log_prob for _, _, log_prob in trajectory)
loss = -(total_log_prob * advantage)

# Step 4: Backprop
loss.backward()
optimizer.step()
```

**Key Properties**:
- âŒ High variance (policy gradient)
- âŒ Can be unstable (negative advantages = "unlearn")
- âŒ Requires careful tuning
- âŒ Struggles on long-horizon tasks

---

## Why TinyZero Succeeds Where new_ragen Fails

### 1. **Task Complexity**
- **TinyZero**: Math problems, single-turn generation
  - Input: "What is 234 Ã— 567?"
  - Output: "Let me calculate... 234 Ã— 567 = 132,678"
  - Reward: Binary (correct=1, wrong=0)

- **new_ragen**: WebShop, multi-turn sequential decisions
  - Turn 1: `search[blue headphones wireless]`
  - Turn 2: `click[B09QKP7XQL]`
  - Turn 3: `click[features]`
  - Turn 4: `buy now`
  - Reward: Sparse (only at end) or dense but noisy

**Impact**: Multi-turn tasks have much longer credit assignment, making REINFORCE's high variance even worse.

### 2. **Curriculum Learning**
- **TinyZero**: Updates difficulty every 25 steps
  - Steps 0-25: Easy problems (small numbers)
  - Steps 26-50: Medium problems
  - Steps 51+: Hard problems

- **new_ragen**: No curriculum
  - All 1000 products from start
  - Complex instructions immediately
  - No gradual difficulty ramp

**Impact**: Without curriculum, model tries to learn impossible tasks before mastering basics.

### 3. **Reward Structure**
- **TinyZero**:
  - Binary rewards for V* computation (0 or 1)
  - Same binary rewards for training
  - Clean signal: "this solution is correct or not"

- **new_ragen**:
  - Dense shaped rewards everywhere
  - Step penalties: -0.1 per action
  - Action bonuses: +0.1 for search, +0.2 for click
  - Final reward only if correct purchase
  - **Problem**: Dense rewards are noisy, can mislead learning

**Impact**: Noisy dense rewards + high-variance REINFORCE = poor learning signal.

### 4. **Advantage Weighting**
- **TinyZero**: Converts to positive weights
  ```python
  weights = (adv_norm + 1.0).clamp(0.1, 5.0)
  # Advantage = -2 â†’ weight = 0.1 (still learn, just less)
  # Advantage = 0  â†’ weight = 1.0 (normal learning)
  # Advantage = +2 â†’ weight = 3.0 (learn more)
  ```

- **new_ragen**: Uses raw advantages
  ```python
  loss = -(log_prob * advantage)
  # Advantage = -2 â†’ loss = +2*log_prob (DECREASE prob)
  # Advantage = 0  â†’ loss = 0 (no learning)
  # Advantage = +2 â†’ loss = -2*log_prob (INCREASE prob)
  ```

**Impact**: TinyZero always makes progress (just weighted), new_ragen can "unlearn" actions.

### 5. **KL Regularization**
- **TinyZero**: Has KL term with reference model
  - Prevents mode collapse
  - Keeps policy diverse
  - Avoids overfitting to high-reward mode

- **new_ragen**: No KL term
  - Policy can collapse to repetitive actions
  - No diversity enforcement
  - Can overfit to spurious patterns

---

## What the Training Logs Tell Us

### TinyZero (Successful Training)
```
Step 25: Loss=2.1453, Reward=0.45, Advantage=0.12
  Curriculum level updated: easy â†’ medium
Step 50: Loss=1.8234, Reward=0.67, Advantage=0.23
  Accuracy: 45% â†’ 68%
Step 100: Loss=1.2145, Reward=0.89, Advantage=0.34
  Accuracy: 68% â†’ 87%
```

### new_ragen (Failed Training)
```
Iteration 1: Reward=-1.100, V*=-1.100, Advantage=0.000
  âš ï¸ All advantages negative - no positive signal!
Iteration 5: Reward=-1.050, V*=-1.100, Advantage=0.050
  Loss: -0.234 (small magnitude)
Iteration 20: Reward=-1.120, V*=-1.100, Advantage=-0.020
  Success rate: 0.0%
```

**Key Observations**:
1. **All V* values negative**: No successful trajectories found in Stage 1
2. **All advantages near zero or negative**: Policy never beats baseline
3. **No learning signal**: Without positive examples, gradient is ~zero
4. **Task too hard**: 3B model can't solve WebShop without curriculum

---

## The Path Forward: 3 Options

### Option A: Quick Fix - Use TinyZero's Algorithm (Recommended) â­

**What**: Replace REINFORCE with weighted SFT approach.

**Pros**:
- âœ… Proven to work with same model
- âœ… More stable training
- âœ… Faster convergence
- âœ… Lower variance

**Cons**:
- âŒ Need to implement teacher-forcing
- âŒ Need reference model
- âŒ More code changes

**Estimated Effort**: 2-3 hours of coding

**Implementation Steps**:
1. Modify `stage2_online.py` to:
   - Generate completions without computing log probs
   - Tokenize prompt + completion
   - Mask prompt tokens in labels
   - Compute per-example CE loss
   - Convert advantages to weights: `(adv_norm + 1.0).clamp(0.1, 5.0)`
   - Apply weights: `(per_ex_ce_loss * weights).mean()`

2. Add reference model:
   - Load same model as policy
   - Freeze weights
   - Use for V* computation and KL regularization

3. Update V* computation:
   - Use smooth max: `V* = max_r + beta * log(mean(exp((r - max_r) / beta)))`

### Option B: Fix REINFORCE Approach (More Work)

**What**: Keep REINFORCE but add missing components.

**Required Changes**:
1. **Add curriculum learning**:
   - Start with 10 products â†’ 100 â†’ 1000
   - Simple instructions first
   - Gradually increase difficulty

2. **Fix reward shaping**:
   - Use binary rewards for V* (like TinyZero)
   - Remove step penalties (too harsh)
   - Add partial credit for progress

3. **Add KL regularization**:
   - Load reference model
   - Compute KL divergence
   - Add to loss: `loss = -log_prob * adv + kl_coef * kl`

4. **Improve exploration**:
   - Higher temperature early (1.5 â†’ 1.0 â†’ 0.8)
   - Entropy bonus in loss
   - Adaptive advantage clipping

**Pros**:
- âœ… True policy gradient (theoretically sound)
- âœ… More flexible (can handle non-differentiable rewards)

**Cons**:
- âŒ Much more tuning required
- âŒ Still high variance
- âŒ Slower convergence
- âŒ May not work even with fixes

**Estimated Effort**: 1-2 days of coding + tuning

### Option C: Hybrid Approach

**What**: Use TinyZero's algorithm + curriculum learning + better rewards.

**Combines**:
- Weighted SFT from TinyZero
- Curriculum learning (10 â†’ 100 â†’ 1000 products)
- Dense partial credit rewards
- Adaptive V* sampling

**Pros**:
- âœ… Best of both worlds
- âœ… Most likely to succeed

**Cons**:
- âŒ Most implementation work

**Estimated Effort**: 3-4 hours

---

## My Recommendation

### ðŸŽ¯ Go with Option A: Switch to TinyZero's Weighted SFT Approach

**Why**:
1. It's **proven to work** with the exact same model (Qwen2.5-3B)
2. **Much more stable** than REINFORCE
3. **Less tuning** required
4. Can **add curriculum later** if needed

**Next Steps**:
1. I'll implement the weighted SFT approach in `stage2_online.py`
2. Add reference model support
3. Update V* computation to use smooth max
4. Test with current WebShop setup
5. If still struggling, add curriculum learning (Option C)

**Expected Results**:
- Stage 1: Should get some positive V* values (> 0)
- Stage 2: Should see steady improvement
- After 100 steps: 10-20% success rate (vs current 0%)
- After 300 steps: 40-60% success rate

---

## Key Takeaway

**The issue was never bugs - it was using the wrong algorithm.**

REINFORCE works for:
- âœ… Simple single-turn tasks
- âœ… Short episodes (< 5 steps)
- âœ… Dense rewards
- âœ… With lots of tuning

WebShop is:
- âŒ Complex multi-turn task
- âŒ Long episodes (10-20 steps)
- âŒ Sparse rewards (only at end)
- âŒ Needs curriculum learning

**Solution**: Use the algorithm that works (TinyZero's weighted SFT), not the one that's theoretically pure (REINFORCE).

---

## Questions?

Ready to implement Option A? I can:
1. Create the new weighted SFT trainer
2. Add reference model support
3. Update training scripts
4. Test on Modal

Let me know if you want to proceed or discuss further!
